\gre
\chapter{Θεωρητικό μέρος - Νευρωνικό δίκτυο}
\label{Chapter3}

\section{Αρχικοποίηση κόστους αντιστοίχησης με χρήση νευρωνικών δικτύων \texorpdfstring{\eng (initialization of matching cost using convolutional neural networks) \gre}{TEXT}}

Από τις παραπάνω εφαρμογές διαφορετικών τεχνικών για την αρχικοποίηση του πίνακα κόστους, εξάγουμε ένα χρήσιμο συμπέρασμα: η απόδοση της μετρικής ομοιότητας μπορεί να βελτιωθεί αν η σύγκριση δεν βασιστεί στις αρχικές τιμές της φωτεινότητας, αλλά χρησιμοποιηθεί ένας πιο αξιόπιστος περιγραφέας της γειτονιάς. Με την λέξη αξιόπιστος περιγράφουμε έναν περιγραφέα που είναι όσο το δυνατόν λιγότερο ευάλωτος σε αλλοιώσεις της "ομοιότητας γειτονιάς", που μπορεί να προκύψει για όλους τους λόγους που αναλύθηκαν στο κεφάλαιο \ref{sec:stereo_constraints_violation}. Για παράδειγμα, ο μετασχηματισμός \eng census \citep{zabih1994non} \gre δημιουργεί τοπικό περιγραφέα που είναι ανεπηρέαστος από τις φωτομετρικές αποκλίσεις. Ταυτόχρονα όμως έχει το μειονέκτημα να δημιουργεί παρόμοιο περιγραφέα από τελείως διαφορετικά είδωλα που τυχαίνει να δημιουργούν γειτονιές με παρόμοια σχέση φωτεινότητας γειτονιάς και κεντρικού \eng pixel. \gre Η παρατήρηση ότι κάθε μέθοδος έχει διαφορετικά πλεονεκτήματα και αδυναμίες προτρέπει τον συνδυασμό μεθόδων στον υπολογισμό του τελικού κόστους για πιο αξιόπιστα αποτελέσματα, όπως επιτυχημένα υλοποιεί η μέθοδος \eng AD-census \gre \citep{mei2011building}.

Το πρόβλημα της εξαγωγής του πιο αξιόπιστου τοπικού περιγραφέα είναι ιδιαίτερα σύνθετο. Αυτή η παρατήρηση ενέπνευσε την αντιμετώπιση του προβλήματος με μεθόδους εκμάθησης μηχανής \eng (machine learning) \gre και πιο συγκεκριμένα με τη χρήση συνελικτικών νευρωνικών δικτύων \eng (convolutional neural networks). \gre Έτσι δίνεται η δυνατότητα στον αλγόριθμο να δημιουργήσει μόνος του τον βέλτιστο περιγραφέα, μαθαίνοντάς τον από τα δεδομένα. Η προϋπόθεση για να συμβεί αυτό είναι η ύπαρξη σετ δεδομένων με στερεοσκοπικά ζεύγη εικόνων που θα περιέχουν την πραγματική πληροφορία παράλλαξης.

Η αρχικοποίηση του κόστους αντιστοίχησης με χρήση συνελικτικών νευρωνικών δικτύων έχει δοκιμαστεί κυρίως τα τελευταία τρία χρόνια, έχοντας αποδώσει εξαιρετικά αποτελέσματα. Οι \eng Zagoruyko, Komodakis \gre \cite{zagoruyko2015learning} πρότειναν τρεις διαφορετικές αρχιτεκτονικές νευρωνικών δικτύων για την σύγκριση τετράγωνων περιοχών εικόνας. \ref{fig:zagoruyko} Οι αρχιτεκτονικές τους εφαρμόστηκαν για την επίλυση προβλημάτων στερεοσκοπικής όρασης μεγάλης απόστασης βάσης \eng(wide baseline).\gre Οι \eng Zbontar, Lecun \gre \citep{zbontar2016stereo} \ref{fig:jzbontar} πρότειναν επίσης δύο αρχιτεκτονικές για την αρχικοποίηση του κόστους σε πρόβλημα μικρής απόστασης βάσης \eng(small baseline).\gre Οι \eng Luo et. al \gre \citep{Luo} \ref{fig:luo}, από την αρχιτεκτονική των οποίων έχει εμπνευστεί κι η παρούσα εργασία, αντιμετώπισαν το πρόβλημα της αρχικοποίησης του κόστους ως πρόβλημα ταξινόμησης πολλαπλών κατηγοριών. Τέλος, οι \eng Alex Kendall et al \gre \citep{kendall2017end} \ref{fig:kendall} και οι \eng Gydaris, Komodakis \gre \citep{gidaris2016detect} \ref{fig:gydaris} αντιμετώπισαν το πρόβλημα του υπολογισμού του χάρτη παράλλαξης με χρήση συνελικτικών νευρωνικών δικτύων από την αρχή ως το τέλος.

\begin{figure}
	\centering
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{Zagoruyko1.png}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{Zagoruyko2.png}
	\end{subfigure}
	\caption{Αρχιτεκτονικές νευρωνικών δικτύων για την σύγκριση περιοχών εικόνας, όπως προτάθηκαν από τους \eng Zagoruyko, Komodakis. \gre \citep{zagoruyko2015learning}}
	
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{jzbontar_fast.png}
	\end{subfigure}
	\label{fig:kendall}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{jzbontar_acc.png}
	\end{subfigure}
	\label{fig:jzbontar}
	\caption{Αρχιτεκτονικές νευρωνικών δικτύων για την σύγκριση περιοχών εικόνας, όπως προτάθηκαν από τους \eng Zbontar, Lecun. \gre \citep{zbontar2016stereo}}
	
	\begin{subfigure}{0.8\textwidth}
		\includegraphics[width=\textwidth]{luo_arch.png}
	\end{subfigure}
	\label{fig:luo}
	\caption{Αρχιτεκτονικές νευρωνικών δικτύων για την σύγκριση περιοχών εικόνας, όπως προτάθηκαν από τους \eng Luo et al. \gre \citep{Luo}}
\end{figure}

\begin{figure}
	\begin{subfigure}{1\textwidth}
		\includegraphics[width=\textwidth]{gidaris_arch.png}
	\end{subfigure}
	\label{fig:gydaris}
	\caption{Αρχιτεκτονικές νευρωνικών δικτύων για την σύγκριση περιοχών εικόνας, όπως προτάθηκαν από τους \eng Gidaris et al. \gre \citep{gidaris2016detect}}
	
	\begin{subfigure}{\textwidth}
		\includegraphics[width=\textwidth]{kendall_arch.png}
	\end{subfigure}
	\label{fig:kendall}
	\caption{Αρχιτεκτονικές νευρωνικών δικτύων για την σύγκριση περιοχών εικόνας, όπως προτάθηκαν από τους \eng Kendall et al. \gre \citep{gidaris2016detect}}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Σετ στερεοσκοπικών δεδομένων με πραγματική πληροφορία παράλλαξης  \texorpdfstring{\eng (stereo dataset with ground-truth disparity) \gre}{TEXT}}
\eng
\subsubsection{Middlebury stereo dataset}
\gre

Tο σετ δεδομένων \eng Middlebury stereo dataset \gre δημιουργήθηκε από το ομώνυμο πανεπιστήμιο των Ηνωμένων Πολιτειών το 2001 \cite{scharstein2002taxonomy}. Έκτοτε η συλλογή έχει ανανεωθεί με νέες εκδόσεις τις χρονολογίες 2003 \citep{scharstein2003high}, 2005 \citep{scharstein2007learning}, 2006 \citep{hirschmuller2007evaluation} και 20014 \citep{scharstein2014high}, περιλαμβάνοντας συνολικά περίπου 50 στερεοσκοπικά ζεύγη εικόνων. Οι διάφορες εκδόσεις εμφανίζουν πολύ μικρές διαφορές μεταξύ τους καθώς όλες χαρακτηρίζονται από τις παρακάτω ιδιότητες, όπως φαίνεται στην εικόνα \ref{fig:middlebury_example}:

\begin{itemize}
\item Οι φωτογραφίες έχουν ληφθεί σε εργαστηριακό περιβάλλον ελεγχόμενου φωτισμού
\item Οι επιφάνειες των αντικειμένων που απαρτίζουν την σκηνή είναι λαμπερτιανές
\item έχουν πλούσια υφή
\item οι αυξομειώσεις του βάθους είναι μικρές, άρα και το σύνολο των τιμών παράλλαξης. Προσεγγιστικά $d \in [10,50] \: pixels$
\item Η πρόσοψη των αντικειμένων εμφανίζει πολύ μικρή κλίση, είναι σχεδόν κάθετη στον άξονα $zz'$ της στερεοσκοπικής διάταξης 
\end{itemize}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{middlebury_example.png}
	\caption{Παραδείγματα εικόνων από την στερεοσκοπική συλλογή \eng middlebury stereo dataset. \gre}
	\label{fig:middlebury_example}
\end{figure}

\eng
\subsubsection{Kitti stereo benchmark}
\gre
Tο σετ δεδομένων \eng Kitti stereo benchmark \gre δημιουργήθηκε το 2012 \cite{geiger2012we} από το τεχνολογικό Ινστιτούτο της Καρλσρούης \eng (Karlsruhe institute of Technology) \gre σε συνεργασία με το τεχνολογικό Ινστιτούτο της Τογιότα στο Σικάγο \eng (Toyota Technological Institute at Chicago). \gre Η συλλογή ανανεώθηκε το 2015 \citep{menze2015object} και περιλαμβάνοντας πλέον συνολικά 400 στερεοσκοπικά ζεύγη εικόνων.

Οι εικόνες έχουν ληφθεί από την οροφή ενός αυτοκινήτου \ref{fig:kitti_car} που κυκλοφορούσε στο κέντρο της Καρλσρούης πρωινές ώρες. Σε κάποιες φωτογραφίες υπάρχει έντονη ηλιοφάνεια και σε άλλες σχετική συννεφιά. Οι δύο κάμερες που είχαν τοποθετηθεί στην οροφή του αυτοκινήτου απέχουν 54 εκατοστά κι οι εικόνες που έλαβαν έχουν ανάλυση $376\times1240$ \eng pixels. \gre Το πραγματικό βάθος των απεικονιζόμενων αντικειμένων έχει ληφθεί από στρεφόμενο σαρωτή λέιζερ \eng (rotating laser scanner) \gre τοποθετημένο πίσω από την αριστερή κάμερα. Ο σαρωτής καταφέρνει να υπολογίσει πληροφορία βάθους για περίπου το $30\% $ του συνόλου των \eng pixels \gre της εικόνας.

Η γεωμετρία και στατιστική των εικόνων που περιλαμβάνει διαφέρει πλήρως από αυτή του \eng Middlebury stereo dataset \gre καθώς χαρακτηρίζεται από τις εξής ιδιότητες:
\begin{itemize}
\item Οι φωτογραφίες έχουν ληφθεί σε φυσικό περιβάλλον, στους δρόμους της Καρλσρούης μια ηλιόλουστη ημέρα. Περιλαμβάνουν κατά κύριο λόγο κινούμενα οχήματα, πεζοδρόμια, πεζούς και σπίτια εκατέρωθεν του δρόμου.
\item Οι επιφάνειες των αντικειμένων που απαρτίζουν την σκηνή είναι δεν είναι στο σύνολό τους λαμπερτιανές. Ταυτόχρονα ο έντονος ήλιος δρα ως μια πολύ δυνατή πηγή φωτός με αποτέλεσμα να δημιουργούνται έντονα φαινόμενα κατοπτρικών ανακλάσεων.\footnote{Στην ανανέωση του 2015 τα τζάμια των αυτοκινήτων περιέχουν πληροφορία παράλλαξης και συμπεριλαμβάνονται στην αξιολόγηση, κάνοντας τη συλλογή πιο απαιτητική στον χειρισμό των κατοπρικών επιφανειών}
\item Η έντονη πηγή φωτός δημιουργεί συχνά κορεσμό στον αισθητήρα $ccd$ με αποτέλεσμα την αποτύπωση ειδώλων χωρίς υφή.
\item οι αυξομειώσεις του βάθους, άρα και το σύνολο των τιμών παράλλαξης, είναι πολύ μεγάλα. Προσεγγιστικά $d \in [0,230] pixels$
\item Οι προσόψεις των αντικειμένων εμφανίζουν μεγάλες κλίση, σε σχέση με τον άξονα $zz'$ της στερεοσκοπικής διάταξης. Ετσί δημιουργούνται εντονότερα φαινόμενα αυξομείωσης αποστάσεων και αποκρύψεων
\end{itemize}

\begin{figure}
	\centering
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{kitti_car.png}
		\caption{Το αυτοκίνητο που χρησιμοποιήθηκε για την συλλογή \eng KITTI. \gre}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{kitti_car_profile.png}
		\caption{Κάτοψη του αυτοκινήτου}
	\end{subfigure}
	\label{fig:kitti_car}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{kitti_example.png}
	\caption{Παραδείγμα εικόνας από την στερεοσκοπική συλλογή \eng kitti stereo benchmark. \gre}
	\label{fig:kitti_example}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}{\textwidth}
		\includegraphics[width=\textwidth]{middlebury_example_1.png}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\includegraphics[width=\textwidth]{kitti_example_1.png}
	\end{subfigure}
	\caption{\textbf{Κίτρινα χρώμα:} μέγιστη κι ελάχιστη τιμή παράλλαξης σε κάθε εικόνα. \textbf{Κόκκινο χωρίο:} διαφορά στην κλίση των εικονιζόμενων αντικειμένων. \textbf{Πράσινο χωρίο:} παράδειγμα κατοπτρικής ανάκλασης. \textbf{Γαλάζιο χωρίο:} παράδειγμα κορεσμού αισθητήρα λόγω έντονου φωτός, με αποτέλεσμα την απεικόνιση επιφάνειας χωρίς υφή}
	\label{fig:midd_kitti_differences}
\end{figure}
\eng
\subsubsection{Synthetic stereo dataset}
\gre
Το 2015, οι \eng Mayer et al. \cite{mayer2016large} \gre δημιούργησαν μια μεγάλη συνθετική συλλογή από στερεοσκοπικά ζεύγη. Η συλλογή αυτή προσεγγίζει σε ομοιότητα φυσικές εικόνες και ταυτόχρονα περιέχει πάρα πολλά στερεοσκοπικά ζεύγη (35.000) αποτελώντας βάση για την εκπαίδευση αλγορίθμων εκμάθησης μηχανής και ιδιαίτερα συνελικτικών νευρωνικών δικτύων. Στην παρούσα εργασία δεν αξιοποιούμε την υπάρχουσα συλλογή.

\begin{figure}
	\centering
	\begin{subfigure}{0.7\textwidth}
		\includegraphics[width=\textwidth]{synthetic_dataset.png}
		\caption{Εικόνα από την συνθετική συλλογή}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\includegraphics[width=\textwidth]{synthetic_kitti_dataset.png}
		\caption{Σύγκριση εικόνων δρόμου συνθετικής συλλογής (δεξιά) και συλλογής \eng kitti stereo benchmark \gre (αριστερά)}
	\end{subfigure}
	\caption{Συνθετική συλλογή στερεοσκοπικών εικόνων}
	\label{fig:synthetic_dataset}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Δημιουργία σετ εκπαίδευσης νευρωνικού δικτύου}

Η δημιουργία του σετ δεδομένων εκπαίδευσης βασίζεται στις εικόνες της στερεοσκοπικής συλλογής \eng KITTI \gre με την ακόλουθη προεπεξεργασία σε κάθε εικόνα:\footnote{όλες οι εικόνες στις οποίες αναφερόμαστε είναι \eng grayscale \gre}
\begin{itemize}
	\item μετατροπή τους σε εικόνες μηδενικής μέσης τιμής. Στην αρχική τους μορφή όλες οι εικόνες είναι πίνακες ακεραίων αριθμών \eng $I_{\mathbf{init}}(\mathbf{p}) \in \mathbb{Z}: \{ 0 \leqslant I(\mathbf{p}) \leqslant 255 \}$ \gre. Εφαρμόζουμε την πράξη \eng $$I_{\mathbf{zero\_mean}}(\mathbf{p}) = I_{\mathbf{init}}(\mathbf{p}) - \textbf{mean}(I_{\mathbf{init}}(\mathbf{p})).$$ \gre Ο νέος πίνακας έχει σύνολο τιμών το σύνολο των πραγματικών αριθμών \eng $I_{\mathbf{zero\_mean}}(\mathbf{p}) \in \mathbb{R}$ \gre
	\item κανονικοποίηση \eng (normalization) \gre μέσω της μετατροπής τους σε εικόνες μοναδιαίας διακύμανσης. Εφαρμόζουμε την πράξη 
\eng $$I_{\mathbf{unit\_variation}}(\mathbf{p}) = \dfrac{I_{\mathbf{zero\_mean}}(\mathbf{p})}{\textbf{standard\_deviation}(I_{\mathbf{zero\_mean}}(\mathbf{p}))}$$ \gre
\end{itemize}

Επισημάνσεις:
\begin{itemize}
	\item Η μετατροπή σε εικόνες μηδενικής μέσης τιμής και μοναδιαίας διακύμανσης γίνεται στο σύνολο της εικόνας κι όχι στο κάθε τετράγωνον χωρίο \eng (patch) \gre που θα εξαχθεί κατά τη δημιουργία του σετ εκπαίδευσης.
	\item Οι υπολογισμοί της μέσης τιμής και της τυπικής απόκλισης γίνεται σε κάθε εικόνα ξεχωριστά. Δεν υπολογίζεται με βάση τον ορισμό της κανονικοποίησης που ορίζει την κανονικοποίηση της κάθε διάστασης (κάθε ξεχωριστό \eng pixel \gre στην προκειμένη) κατά μήκος όλου του σετ εκπαίδευσης. Αυτή η εναλλακτική μορφή κανονικοποίησης είναι η πιο διαδεδομένη μέθοδος προεπεξεργασίας όταν η συλλογή περιλαμβάνει εικόνες.
\end{itemize}

\begin{comment}
\subsection{Δημιουργία σετ δεδομένων δυαδικής ταξινόμησης \texorpdfstring{\eng (binary classification dataset) \gre}{TEXT}}

Συμβολίζουμε με
\eng
$
<\mathcal{P}_{n \times n}^L(\mathbf{p}), \mathcal{P}_{C, n \times n}^R(\mathbf{q_1}), \mathcal{P}_{W, n \times n}^R(\mathbf{q_2})>
$
\gre
μια εγγραφή του σετ εκπαίδευσης. 

Οι εγγραφές δημιουργούνται με την ακόλουθη μεθοδολογία. Για κάθε σημείο \eng $\textbf{p} \in I^L$ \gre όπου η τιμή της παράλλαξης είναι γνωστή:
\begin{itemize}
	\item Σαρώνουμε κάθε εικόνα της στερεοσκοπικής συλλογής μέχρι να συναντήσουμε θέση \eng $\textbf{p}$ \gre με γνωστή παράλλαξη 
	\item Όταν το βρούμε τέτοια θέση \eng $\textbf{p}$ \gre αποκόβουμε τετράγωνο χωρίο διαστάσεων $n \times n$ \eng pixels \gre πέριξ αυτής και το αποθηκεύουμε ως \eng $\mathcal{P}_{n \times n}^L(\mathbf{p})$ \gre
	\item Αποκόβουμε αντίστοιχο χωρίο στη θέση \eng $\mathbf{q_1}$ \gre της έτερης εικόνας $I^L$ και το ονομάζουμε \eng $\mathcal{P}_{C, n \times n}^R(\mathbf{q_1})$ \gre. Η θέση \eng $\mathbf{q_1}$ \gre υπολογίζεται ως 
	\eng $$\mathbf{q_1} = \texttt{int}(x - d \pm o_{\text{cor}}, y)$$ \gre , όπου $d$ η πραγματική τιμή της παράλλαξης του σημείου. Ο όρος \eng $\pm o_{\text{cor}} \in [-0.5, 0.5]$ \gre προστίθεται διότι πειραματικά παρατηρήθηκε να βελτιώνει το αποτέλεσμα. Πιο συγκεκριμένα, ο αλγόριθμος άθροισης κόστους σε προσαρμόσιμη περιοχή υποστήριξης που θα αναλυθεί σε επόμενο κεφάλαιο αποδίδει καλύτερα όταν ο πίνακας κόστους περιέχει χαμηλές τιμές τόσο στην αντίστοιχη περιοχή όσο και στις γειτονικές αυτής. Από γεωμετρικής άποψης βασίζεται στο ότι η τιμή φωτεινότητας που καταγράφεται στον αισθητήρα \eng ccd \gre κοντά στην ευθεία που διαχωρίζει δύο \eng pixels \gre έχει πιθανά αποδοθεί και στα δύο \eng pixels. \gre
	\item Αποκόβουμε αντίστοιχο χωρίο στη θέση \eng $\mathbf{q_2}$ \gre της έτερης εικόνας $I^L$ και το ονομάζουμε \eng $\mathcal{P}_{W, n \times n}^R(\mathbf{q_1})$ \gre. Η θέση \eng $\mathbf{q_2}$ \gre υπολογίζεται ως \eng $$\mathbf{q_2} = \texttt{int}(x - d \pm o_{\text{wrong}}, y)$$ \gre όπου $d$ η πραγματική τιμή της παράλλαξης του σημείου. Ο όρος \eng $\pm o_{\text{wrong}}$ \gre λαμβάνει τιμές στο διάστημα \eng $[\texttt{dataset\_wrong\_min}, \texttt{dataset\_wrong\_max}]$. \gre Οι τιμές \eng $\texttt{dataset\_wrong\_min}$\gre και \eng $\texttt{dataset\_wrong\_max}$ \gre αποτελούν υπερπαράμετρους, όπως φαίνεται απ' την γραμματοσειρά τους, κι η επιλογή των τιμών τους προκύπτει πειραματικά.
\end{itemize}

Η παραπάνω διαδικασία φαίνεται στο σχήμα \ref{kitti2012_binary_dataset}. Όπως προδίδει ο συμβολισμός \eng $\mathcal{P}_{n \times n}^L(\mathbf{p})$ \gre είναι το χωρίο αναφοράς, \eng $\mathcal{P}_{C, n \times n}^R(\mathbf{q_1})$ \gre το αντίστοιχο χωρίο του στην έτερη εικόνα και \eng $\mathcal{P}_{W, n \times n}^R(\mathbf{q_2})$ \gre ένα άλλο χωρίο μετατοπισμένο κατά \eng $\pm o_{\text{wrong}}$ \gre επί της επιπολικής ευθείας (θα το αναφέρουμε για συντομία με την αδόκιμη ονομασία "εσφαλμένο χωρίο").

Με την παραπάνω μεθοδολογία εξασφαλίζουμε την ύπαρξη ίσης ποσότητας αντίστοιχων και εσφαλμένων χωρίων στο σύνολο του τεστ εκπαίδευσης.

Δημιουργούνται περίπου $10^5\times\dfrac{\text{εγγραφές}}{\text{εικόνα}}$ οπότε το συνολικό μέγεθος του σετ εκπαίδευσης είναι περίπου $4\times10^7$ παραδείγματα, μόνο από την συλλογή \eng KITTI. \gre

Ο λόγος που επιλέγουμε το "εσφαλμένο χωρίο" με μετατόπιση κατά μήκος της επιπολικής ευθείας, κι όχι σε μια τυχαία κατεύθυνση, βασίζεται στην επιθυμία μας το δίκτυο να "μάθει" να διαχωρίζει χωρία κατά μήκος της επιπολικής ευθείας μόνο καθώς αυτές είναι οι μοναδικές θέσεις που αναζητούμε το αντίστοιχο σημείο λόγω του στερεοσκοπικού περιορισμού.

\subsection{Δημιουργία σετ δεδομένων ταξινόμησης πολλαπλών κατηγοριών \texorpdfstring{\eng (multi-class classification dataset) \gre}{TEXT}}
\end{comment}

Συμβολίζουμε με

\eng
$$
<\mathcal{P}_{n \times n}^L(\mathbf{p}), \mathcal{P}_{( \texttt{max\_disparity} + n) \times n}^R(\mathbf{q}), \text{label}>
$$
\gre

κάθε εγγραφή του σετ εκπαίδευσης. 

Οι εγγραφές δημιουργούνται με την ακόλουθη μεθοδολογία. Σε κάθε σημείο \eng $\textbf{p} = (x,y) \in I^L$ \gre όπου ισχύουν οι τρεις παρακάτω προϋποθέσεις:
\begin{itemize}
	\item η τιμή της παράλλαξης είναι γνωστή
	\item \eng $\texttt{max\_disparity} + \dfrac{n-1}{2} < x < \text{width} - \dfrac{n-1}{2}$ \gre, 
	\item \eng $\dfrac{n-1}{2} < y < \text{height} - \dfrac{n-1}{2}$ \gre
\end{itemize}

εφαρμόζουμε την εξής μεθοδολογία:

\begin{itemize}
	\item Αποθηκεύουμε το τετράγωνο χωρίο διαστάσεων $n \times n$ \eng pixels \gre πέριξ του σημείου \eng $\textbf{p}$ \gre ως \eng $\mathcal{P}_{n \times n}^L(\mathbf{p})$. \gre 
	\item Αποθηκεύουμε το παραλληλόγραμμο χωρίο διαστάσεων \eng $( \texttt{max\_disparity} + n) \times n$ \gre ως \eng $\mathcal{P}_{( \texttt{max\_disparity} + n) \times n}^R(\mathbf{q})$ \gre. Η θέση \eng $\mathbf{q}$ \gre υπολογίζεται ως 
	\eng $$\mathbf{q} = \texttt{int}(x - d, y)$$ \gre και το χωρίο εκτείνεται:
	\begin{itemize}
		\item $\dfrac{n-1}{2}$ θέσεις προς τα πάνω, κάτω και δεξιά
		\item \eng $\texttt{max\_disparity} + \dfrac{n-1}{2}$ \gre θέσεις προς τα αριστερά
	\end{itemize}
	\item Αναθέτουμε ως \eng label \gre την τιμή της παράλλαξης του σημείου. Η ετικέτα \eng label \gre λαμβάνει τιμές στο διάστημα \eng $[0,\texttt{max\_disparity}]$ \gre
\end{itemize}
Η παραπάνω διαδικασία φαίνεται στο σχήμα \ref{kitti2012_multi_class_dataset}. Δημιουργούνται περίπου $8\cdot10^4\times\dfrac{\text{εγγραφές}}{\text{εικόνα}}$ οπότε το συνολικό σετ εκπαίδευσης περιέχει περίπου $32\times10^6$ εγγραφές.
\begin{figure}
	\centering
	\begin{subfigure}{\textwidth}
	\includegraphics[width=\textwidth]{kitti2012_im9.png}
	\end{subfigure}
	\label{fig:kitti2012_imL_9}
	\caption{αριστερή εικόνα στερεοσκοπικού ζεύγους της συλλογής \eng kitti (2012) \gre}
	
%	\begin{subfigure}{0.32\textwidth}
%		\includegraphics[width=\textwidth, height=\textwidth]{ref_crop.png}
%		\caption{\eng $\mathcal{P}_{150 \times 150}^L(731,163)$ \gre}
%	\end{subfigure}
%	\begin{subfigure}{0.32\textwidth}
%		\includegraphics[width=\textwidth, height=\textwidth]{right_crop_cor.png}
%		\caption{\eng $\mathcal{P}_{C, 150 \times 150}^R(701,163)$ \gre}
%	\end{subfigure}
%		\begin{subfigure}{0.32\textwidth}
%		\includegraphics[width=\textwidth, height=\textwidth]{right_crop_wrong.png}
%		\caption{\eng $\mathcal{P}_{W, 150 \times 150}^R(681,163)$ \gre}
%	\end{subfigure}
%	\label{kitti2012_binary_dataset}
%	\caption{παράδειγμα δημιουργίας εγγραφής στο σετ εκπαίδευσης δυαδικής ταξινόμησης}
	
	\begin{subfigure}{.3\textwidth}
		\includegraphics[width=\textwidth, height=4cm]{kitti2012_im9_lcrop.png}
		\caption{\eng $\mathcal{P}_{150 \times 150}^L(163,731)$ \gre}
	\end{subfigure}
		\begin{subfigure}{.69\textwidth}
		\includegraphics[width=\textwidth, height=4cm]{kitti2012_im9_rcrop.png}
		\caption{\eng $\mathcal{P}_{( 150 + 150) \times 150}^R(163,701)$ \gre}
	\end{subfigure}
	\label{kitti2012_multi_class_dataset}
	\caption[αεκ]{παράδειγμα δημιουργίας εγγραφής στο σετ εκπαίδευσης ταξινόμησης πολλαπλών κατηγοριών. H επιλογή μεγέθους ορθογώνιου χωρίου $150\times150$ δεν είναι ρεαλιστική, αλλά έγινε για να είναι εύληπτη η εικόνα. Στην πραγματικότητα το μέγεθος του χωρίου δεν υπερέβη ποτέ τα \eng $n=40 \text{pixels}$ \gre στους πειραματισμούς μας.}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
\subsection{Επαύξηση του δυαδικού σετ εκπαίδευσης \texorpdfstring{\eng (dataset augmenation) \gre}{TEXT}}

Τα δημιουργημένα σετ εκπαίδευσης πρέπει να περιλαμβάνουν όσο το δυνατόν μεγαλύτερη ποικιλία φαινομένων που επιδρούν στην στερεοσκοπική όραση (κοντινά/μακρινά αντικείμενα, πολλά/λίγα αντικείμενα, κάθετα/υπό γωνία στον άξονα $zz'$, διαφορετικές γεωμετρίες στερεοσκοπικής διάταξης, έντονη/ελλιπής φωτεινότητα, πολλές/λίγες αποκρύψεις/φωτομετρικές αλλοιώσεις/επαναλαμβανόμενα μοτίβα/επιφάνειες χωρίς υφή κ.α.). Όσο μεγαλύτερη περιπτωσιολογία φαινομένων στερεοσκοπικής όρασης περιλαμβάνει η συλλογή του σετ εκπαίδευσης, τόσο πιθανότερη η απόκτηση ικανότητας του δικτύου να τα "χειρίζεται". Δημιουργώντας το σετ εκπαίδευσης από μία μόνο συλλογή εικόνων \eng(KITTI)\gre μειώνουμε αυτήν την δυνατότητα, κερδίζοντας τον πειραματισμό σε ένα λίγο διαφορετικό ερώτημα: μπορεί το δίκτυο να "γενικεύσει" τη γνώση του σε ευρύτερα σύνολα παραδειγμάτων (στερεοσκοπικά ζεύγη) στα οποία δεν έχει αμιγώς εκπαιδευτεί; Γι' αυτό επιλέγουμε την εκπαίδευση στην πλέον απαιτητική συλλογή, ενώ η επικύρωση θα γίνει σε ευρύτερο σύνολο εικόνων.

Ταυτόχρονα εφαρμόζουμε την τεχνική επαύξησης του σετ εκπαίδευσης, ώστε να βοηθήσουμε το δίκτυο στη "γενίκευση"\footnote{Στους αλγόριθμους εκμάθησης μηχανής με τον όρο γενίκευση περιγράφουμε την ικανότητα του αλγορίθμου να εμφανίζει ταύτιση επιδόσεων ανάμεσα στα παραδείγματα του σετ εκπαίδευσης και στα υπόλοιπα παραδείγματα που θα καλεστεί να αντιμετωπίσει.}  των προβλέψεών του. Κατά την επαύξηση, μετασχηματίζουμε τα παραδείγματα (χωρία εικόνων) προτού τα προωθήσουμε στο δίκτυο. Οι παράμετροι του μετασχηματισμού επιλέγονται τυχαία κι άρα ο μετασχηματισμός είναι διαφορετικός για κάθε παράδειγμα που προωθείται στο δίκτυο. Μάλιστα επιλέγουμε ο μετασχηματισμός να είναι ελαφρά διαφοροποιημένος ανάμεσα στο αριστερό και το δεξίο χωρίο. Συγκεκριμένα εφαρμόζουμε τους εξής μετασχηματισμούς: \eng

\begin{itemize}
\item Rotate the left patch by \textit{rotate} degrees and the right patch by
\(\textit{rotate} + \textit{rotate\_diff}\) degrees.

\item Scale the left patch by \textit{scale} and the right patch by 
\(\textit{scale} \cdot \textit{scale\_diff} \).
\item Scale the left patch in the horizontal direction by \textit{horizontal\_scale} and
the right patch by \(\textit{horizontal\_scale} \cdot \textit{horizontal\_scale\_diff}\).
\item Shear the left patch in the horizontal direction by \textit{horizontal\_shear} and
the right patch by \(\textit{horizontal\_shear} + \textit{horizontal\_shear\_diff}\).
\item Translate the right patch in the vertical direction by \textit{vertical\_disparity}.
\item Adjust the brightness and contrast by setting the left and right image patches to:
\begin{align*}
\mathcal{P}^L &\leftarrow \mathcal{P}^L \cdot \textit{contrast} + \textit{brightness} \text{ and} \\
\mathcal{P}^R &\leftarrow \mathcal{P}^R \cdot (\textit{contrast} \cdot \textit{contrast\_diff}) + 
(\textit{brightness} + \textit{brightness\_diff}),
\end{align*}
with addition and multiplication carried out element-wise where appropriate.

\end{itemize}

\begin{table}[tb]
\begin{center}
\begin{tabular}{l cc cc}\toprule

& \multicolumn{2}{c}{KITTI 2012} &
\multicolumn{2}{c}{Middlebury}\\
\cmidrule(lr){2-3}
\cmidrule(lr){4-5}
Hyperparameter & Range & Error & Range & Error \\\midrule

\texttt{rotate} & \([-7, 7] \) & 2.65 & \([-28, 28]\) & 7.99 \\
\texttt{scale} & & & \([0.8, 1]\) & 8.17 \\
\texttt{horizontal\_scale} & \([0.9, 1]\) & 2.62 & \([0.8, 1]\) & 8.08 \\
\texttt{horizontal\_shear} & \([0, 0.1]\) & 2.61 & \([0, 0.1]\) & 7.91 \\
\texttt{brightness} & \([0, 0.7]\) & 2.61 & \([0, 1.3]\) & 8.16 \\
\texttt{contrast} & \([1, 1.3]\) & 2.63 & \([1, 1.1]\) & 7.95 \\
\texttt{vertical\_disparity} & & & \([0, 1]\) & 8.05 \\
\texttt{rotate\_diff} & & & \([-3, 3]\) & 8.00 \\
\texttt{horizontal\_scale\_diff} & & & \([0.9, 1]\) & 7.97\\
\texttt{horizontal\_shear\_diff} & & & \([0, 0.3]\) & 8.05\\
\texttt{brightness\_diff} & \([0, 0.3] \) & 2.63 & \([0, 0.7]\) & 7.92 \\
\texttt{contrast\_diff} & & & \([1, 1.1]\) & 8.01 \\\midrule
No data set augmentation & & 2.73 & & 8.75 \\
Full data set augmentation & & 2.61 & & 7.91 \\\bottomrule
\end{tabular}
\caption{The hyperparameters governing data augmentation and how they affect
the validation error. The ``Error'' column reports the validation error when a
particular data augmentation step is not used. The last two rows report
validation errors with and without data augmentation. For example, the
validation error on the KITTI 2012 is 2.73\,\% if no data augmentation is used,
2.65\,\% if all steps except rotation are used, and 2.61\,\% if all data
augmentation steps are used. }

\label{tbl:da_params}
\end{center}
\end{table}
\end{comment}
\gre


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Αρχιτεκτονική συνελικτικού νευρωνικού δικτύου}

Προσεγγίζουμε το πρόβλημα της αρχικοποίησης του πίνακα κόστους, ως πρόβλημα ταξινόμησης πολλαπλών κατηγοριών. Κάθε \eng pixel \gre $\mathbf{p}$ της λήψης αναφοράς αντιστοιχίζεται (ταξινομείται) σε μία τιμή (κατηγορία) του συνόλου $\{0,1,2,\ldots,\mathtt{max\_disparity}\}$. Το νευρωνικό δίκτυο χωρίζεται σε δύο μέρη, το συνελικτικό δίκτυο στο οποίο γίνεται η εξαγωγή του τοπικού περιγραφέα και το δίκτυο απόφασης στο οποίο υπολογίζεται η τιμή του κόστους ομοιότητας.

\subsection{Εξαγωγή τοπικών περιγραφέων - Συνελικτικό νευρωνικό δίκτυο}

Το συνελικτικό νευρωνικό δίκτυο δέχεται ως είσοδο ένα τετράγωνο χωρίο διάστασης $[\mathtt{patch\_size} \times \mathtt{patch\_size} \times 1]$ \eng pixels. \gre

Κάθε μπλοκ του συνελικτικού νευρωνικού δικτύου απαρτίζεται από τα εξής επίπεδα:
\begin{itemize}
	\item Δισδιάστατη συνέλιξη με $\mathtt{num\_conv\_feature\_maps}$ φίλτρα διαστάσεων $[\mathtt{kernel\_size} \times \mathtt{kernel\_size} \times \mathtt{num\_conv\_feature\_maps}]$\footnote{Εκτός από το πρώτο μπλοκ, στο οποίο το φίλτρο συνέλιξης έχει διάσταση $\mathtt{kernel\_size} \times \mathtt{kernel\_size} \times 1$, καθώς δέχεται ως είσοδο \eng grayscale \gre τετράγωνο χωρίο διάστασης $\mathtt{patch\_size} \times \mathtt{patch\_size} \times 1$}. Η συνέλιξη εφαρμόζεται χωρίς \eng zero padding \gre στον πίνακα εισόδου. Επομένως, αν ο πίνακας εισόδου έχει διαστάσεις $[\mathtt{patch\_size} \times \mathtt{patch\_size} \times \mathtt{num\_conv\_feature\_maps}]$, η έξοδος της πράξης θα είναι πίνακας διαστάσεων $[(\mathtt{patch\_size}-\mathtt{kernel\_size}+1) \times (\mathtt{patch\_size}-\mathtt{kernel\_size}+1) \times \mathtt{num\_conv\_feature\_maps}]$. Στο αποτέλεσμα της συνέλιξης του σήματος εισόδου με το κάθε φίλτρο προστίθεται ένας όρος (πόλωση), μετατρέποντας την συνολική πράξη σε μετασχηματισμό \eng affine. \gre Συμβολίζοντας με $x$ τον πίνακα εισόδου, $h_i$ το κάθε φίλτρο, $b_i$ την κάθε πόλωση και $y_i$ το αποτέλεσμα του μετασχηματισμού:
	
	$$y_i = \mathbf{conv2d}(x,h_i) + b_i$$
	ο συνολικός τρισδιάστατος πίνακας εξόδου $y$ του επιπέδου, είναι η παράθεση 
	$\mathtt{num\_conv\_feature\_maps}$ δισδιάστατων πινάκων $y_i$ κατά την τρίτη διάσταση:
	\eng
	\begin{verbbox}
		for i = 1:num_conv_feature_maps
		    y[:,:,i] = y_i	
	\end{verbbox}
	\gre
	\begin{figure}[ht]
  		\centering
  		\theverbbox
	\end{figure}

	\item Κανονικοποίηση δέσμης \eng (batch normalization): \citep{ioffe2015batch} \gre
	\begin{itemize}
		\item Είσοδος: οι τρισδιάστατοι πίνακες $y_{\mathbf{conv2d}}$ όλης της δέσμης εκπαίδευσης.
		\item 'Εξοδος: οι τρισδιάστατοι πίνακες $y_{\mathbf{BN}}$ όλης της δέσμης εκπαίδευσης, κανονικοποιημένοι με βάση το κάθε ξεχωριστό \eng feature map. \gre
		\item Διαδικασία: Σε κάθε βήμα εκπαίδευσης, προωθούνται στο δίκτυο $\mathtt{batch\_size}$ εγγραφές του σετ εκπαίδευσης. Κάθε εγγραφή, έχοντας περάσει από το επίπεδο της δισδιάστατης συνέλιξης αναπαρίσταται από έναν πίνακα $y_{conv2d}$. Ο τρισδιάστατος πίνακας $y_{conv2d}$ απαρτίζεται από $\mathtt{num\_conv\_feature\_maps}$ δισδιάστατους πίνακες $y_{conv2d,i}$. Δημιουργούμε $\mathtt{num\_conv\_feature\_maps}$ ομάδες, μέλη της οποίας είναι κατ' αντιστοιχία, όλοι οι $\mathtt{batch\_size}$ πίνακες $y_{conv2d,i}$. Τις ομάδες αυτές τις συμβολίζουμε ως $X_j$ και κάθε ξεχωριστό στοιχείου τους ως $X_j^{i}$. Επί αυτών των ομάδων εφαρμόζεται η κανονικοποίηση. Συγκεκριμένα, η κανονικοποίηση δέσμης $X_{\mathbf{BN}, j} = BN_{\gamma, \beta}(X_j) $ υλοποιείται με την ακόλουθη μεθοδολογία:
		
		\begin{equation} \label{eq:batch_normalization}
			\centering
			\begin{split}
				\mu_{X_j} &= \sfrac{1}{m} \sum_{i=1}^{m} X_j^{i} \qquad \qquad \text{// μέσος όρος ομάδας}\\
				\sigma_{X_j}^2 &= \sfrac{1}{m} \sum_{i=1}^{m} (X_j^{i} - \mu_{X_j})^2 \qquad \qquad \text{// διακύμανση ομάδας}\\
				\hat{X_j^{i}} &= \dfrac{X_j^{i} - \mu_{X_j}}{\sqrt{\sigma_{X_j}^2 + \epsilon}} \qquad \qquad \text{// κανονικοποίηση}\\
				\hat{X_j^{i}} &= \gamma \hat{X_j^{i}} + \beta \qquad \qquad \text{// κλιμάκωση και μετατόπιση}
			\end{split}
		\end{equation}
		
Οι μετασχηματισμένες τιμές $\hat{X_j^{i}}$ "αποστοιχίζονται" από τις ομάδες τους κι αναδιατάσσονται στη δομή που είχαν κατά την είσοδό τους στο επίπεδο. Έτσι συνολικά έχει επιτευχθεί μια πράξη $y_{\mathbf{BN}} = ΒΝ(y_{\mathbf{conv2d}})$.
	\end{itemize}
	\item Συνάρτηση γραμμικού ανορθωτή \eng (Rectified Linear Unit - ReLU):\gre Συνίσταται στην πράξη \eng $$\text{ReLU}(x) = max(0,x)$$\gre μηδενίζοντας όλες τις αρνητικές τιμές που δέχεται κι αφήνοντας αμετάβλητες τις υπόλοιπες.
\end{itemize}

Η παραπάνω δομή επιπέδων επαναλαμβάνεται διαδοχικά $\mathtt{num\_conv\_layers}$ φορές. Έπειτα από κάθε μπλοκ, οι χωρικές διαστάσεις του χωρίου μειώνονται κατά $\dfrac{kernel\_size - 1}{2}$. Στο τέλος ολόκληρου του συνελικτικού νευρωνικού δικτύου, ο πίνακας που θα προκύψει θα έχει διαστάσεις $[1 \times 1 \times \mathtt{num\_conv\_feature\_maps}]$ κι ουσιαστικά θα είναι ο τοπικός περιγραφέας του χωρίου που δόθηκε ως είσοδος στο δίκτυο.

Συμπεράσματα:

\subsection{Δίκτυο απόφασης}

Το συνελικτικό νευρωνικό δίκτυο εξάγει έναν τοπικό περιγράφεα του χωρίου που δόθηκε ως είσοδος. Ο περιγραφέας είναι ένα διάνυσμα διάστασης $\mathtt{num\_conv\_feature\_maps}$:

$$\mathbf{Ι_{descriptor}} \in \mathbb{R}^{\mathtt{num\_conv\_feature\_maps}}$$

Στόχος του δικτύου απόφασης είναι η εκτίμηση της ομοιότητας των δύο περιγραφέων, που αναλογούν στα δύο υπό σύγκριση χωρία. Η εκτίμηση αυτή είναι το αποτέλεσμα μια συνάρτησης $f$, που θα δέχεται ως είσοδο τους δύο τοπικούς περιγραφείς $\mathbf{Ι}_{descriptor}^L$, $\mathbf{Ι}_{descriptor}^R$ και θα επιστρέφει μια εκτίμηση ομοιότητας:
	
	$$f: \mathbb{R}^{2 \times \mathtt{num\_conv\_feature\_maps}} \rightarrow \mathbb{R}$$
	$$similarity = f(\mathbf{Ι}_{descriptor}^L, \mathbf{Ι}_{descriptor}^R)$$

Η επιλογή της συνάρτησης αυτής μπορεί να γίνει με δύο τρόπους:

\begin{itemize}
	\item Με την εφαρμογή μιας προαποφασισμένης συνάρτησης $f$ όπως η μέση απόλυτη διαφορά, η μέση τετραγωνική διαφορά, το εσωτερικό γινόμενο ή η ομοιότητα συνημιτόνου.
	\item Με την χρήση μηχανικής μάθησης για την εκμάθηση της βέλτιστης συνάρτησης $f$ από τα δεδομένα. Στην περίπτωση, μια ενδεδειγμένη λύση είναι η εκπαίδευση ενός "πλήρως συνδεδεμένου" \eng (fully connected) \gre τεχνητού νευρωνικού δικτύου \eng (artificial neural net). \gre
\end{itemize}

Πλεονεκτήματα/Μειονεκτήματα της χρήσης τεχνητού νευρωνικού δικτύου ως δίκτυο απόφασης:

\begin{itemize}
	\item Η εκπαίδευση ενός νευρωνικού δικτύου για την εκτίμηση της ομοιότητας αποτελεί την βέλτιστη επιλογή με κριτήριο την ακρίβεια. Η επιλογή αυτή δίνει τη δυνατότητα στο δίκτυο να "μάθει" από τα δεδομένα την βέλτιστη συνάρτηση $f$ που θα περατώνει τον σκοπό της εκτίμησης ομοιότητας των δύο διανυσμάτων εισόδου. 
	\item Από πλευράς ταχύτητας, η βέλτιστη επιλογή είναι η χρήση μιας προαποφασισμένης συνάρτησης, όπως για παράδειγμα το εσωτερικό γινόμενο. Η σύγκριση εκτελείται σειριακά $\mathbf{max\_disparity} + 1$ φορές για κάθε σημείο της εικόνας, επομένως αν ο χρόνος περάτωσης της συνάρτησης σύγκρισης είναι $t_f$, ο συνολικός χρόνος θα είναι $(\mathbf{max\_disparity} + 1)\times t_f$. Η χρήση μιας απαιτητικής υπολογιστικά συνάρτησης με πολύ μεγάλο $t_f$, όπως το "απόλυτα συνδεδεμένο" νευρωνικό δίκτυο αυξάνει πολύ έντονα το χρόνο εκτέλεσης. Αντιθέτως, μια συνάρτηση όπως το εσωτερικό γινόμενο, αφενός λόγω πολύ μικρού $t_f$ κρατάει τον συνολικό χρόνο σε χαμηλά επίπεδα ακόμη και για μεγάλο $\mathbf{max\_disparity}$ αφετέρου μπορεί να υλοποιηθεί παράλληλα σε κάρτα γραφικών.
	\item Σε κάθε περίπτωση, η εκπαίδευση του νευρωνικού δικτύου, συνελικτικού μέρους και δικτύου απόφασης, γίνεται ενιαία. Αυτό προυποθέτει τη χρήση μιας παραγωγίσιμης συνάρτησης $f$, για να μπορεί να εφαρμοστεί πάνω της ο κανόνας της οπισθοδιάδοσης \eng(back propagation). \gre
\end{itemize}

Παρατηρώντας ότι τα αποτελέσματα είναι ικανοποιητικά με χρήση ενός εσωτερικού γινομένου στα δύο διανύσματα, με ταυτόχρονη έντονη μείωση της υπολογιστικής πολυπλοκότητας το προτιμούμε σε σχέση με την εκπαίδευση ενός "απόλυτα συνδεδεμένου" νευρωνικού δικτύου.

\subsection{Εκπαίδευση}

Όπως αναφέρθηκε κάθε εγγραφή του σετ εκπαίδευσης συμβολίζεται ως

\eng
$$
<\mathcal{P}_{n \times n}^L(\mathbf{p}), \mathcal{P}_{( \texttt{max\_disparity} + n) \times n}^R(\mathbf{q}), \text{label}>
$$
\gre


Σε κάθε βήμα εκπαίδευσης προωθούμε στο δίκτυο μια δέσμη \eng (batch) \gre $\mathbf{batch\_size}$ εγγραφών. Τα δύο χωρία $\mathcal{P}_{n \times n}^L(\mathbf{p})$ και $\mathcal{P}_{( \texttt{max\_disparity} + n) \times n}$ προωθούνται ταυτόχρονα στον κάθε κλάδο του σιαμαίου δικτύου. Κατά την έξοδο τους από το συνελικτικό μέρος, αναπαρίστανται από δύο πίνακες διαστάσεων $\mathtt{num\_conv\_feature\_maps}$ και $(\mathbf{max\_disparity} + 1)\times \mathtt{num\_conv\_feature\_maps}$. Εφαρμόζουμε τξν πράξη του εσωτερικού γινομένου $(\mathbf{max\_disparity} + 1)$, υπολογίζοντας έτσι το διάνυσμα $\mathbf{score} \in \mathbb{R}^\mathtt{num\_conv\_feature\_maps}$. Το διάνυσμα $\mathbf{score}$ περιέχει το σκορ ομοιότητας του χωρίου αναφοράς με το αντίστοιχο χωρίο της έτερης λήψης, σε κάθε πιθανή θέση παράλλαξης.

Η μετατροπή των τιμών του διανύσματος $\mathbf{score} \in \mathbb{R}^\mathtt{max\_disparity}$ σε πιθανότητες γίνεται μέσω της συνάρτησης \eng softmax: \gre

$$p_d = \dfrac{e^{\mathbf{score}_d}}{\sum_{d = 0}^{\mathtt{max\_disparity}} e^{\mathbf{score}_d} }, \quad \forall d \in [0, \mathtt{max\_disparity}] $$

Μέσω αυτής της πράξης δημιουργείται ένα διάνυσμα $\mathbf{poss} \in \mathbb{R}^\mathtt{max\_disparity}$, με τιμές εντός του διανύσματος $[0,1]$ το άθροισμα των οποίων ισούται με την μονάδα $\sum_{d=0}^\mathtt{max\_disparity} p_d = 1$, δηλαδή ένα διάνυσμα πιθανοτήτων.

Ορίζουμε συνάρτηση εντροπίας ως:

$$H = \sum_{d = 0}^{\mathtt{max\_disparity}} P_{gt}(\mathbf{poss}_d) log (\mathbf{poss}_d)$$

Εφόσον υπάρχει μια ανοχή στην πρόβλεψη παράλλαξης που απέχει μέχρι και $< 3 \: px$ από την πραγματική, δημιουργούμε την κατανομή $P_{gt}(poss_d)$ με βάση τον ακόλουθο κανόνα:

\begin{equation*}
	P_{gt}(poss_d) = \begin{cases}
		0.5 & \text{εάν $poss_d =$ \eng label}\\
		0.2 & \text{εάν} \: |poss_d - \text{\eng label}| = 1\\
		0.05 & \text{εάν} \: |poss_d - \text{\eng label}| = 2\\
		0 & \text{αλλιώς}
	\end{cases}
\end{equation*}

Αυτή η κατανομή $P_{gt}(poss_d)$ είναι μια παραλλαγή της τυπικής συνάρτησης εντροπίας στα προβλήματα ταξινόμησης που ορίζεται ως μια συνάρτηση δέλτα, με την μονάδα στην τιμή της σωστής κατηγορίας.


Στόχος της εκπαίδευσης είναι η ρύθμιση όλων των μεταβλητών παραμέτρων του δικτύου (βάρη των φίλτρων, πολώσεις, συντελεστές $\gamma, \beta$ της κανονικοποίησης δέσμης) για την ελαχιστοποίηση της τιμής της εντροπίας. 

\subsubsection{Αρχικοποίηση των εκπαιδεύσιμων παραμέτρων του δικτύου}

Αναπαρίστουμε την συνάρτηση κόστους του κάθε παραδείγματος εκπαίδευσης ως $H_j$. Σε κάθε βήμα εκπαίδευσης το συνολικό κόστος υπολογίζεται στο σύνολο της δέσμης εκπαίδευσης:

$$loss = \sum_{j = 0}^{\mathtt{batch\_size}} H_j$$

κι είναι μια τιμή που εξαρτάται από:
\begin{itemize}
	\item τις εκπαιδεύσιμες παραμέτρους του δικτύου, έστω ότι τις συμβολίζουμε με $W$
	\item τα παραδείγματα της δέσμης εκπαίδευσης με τις αντίστοιχες ετικέτες τους \eng(labels)\gre, έστω ότι τα συμβολίζουμε ως $X$
\end{itemize}

Θεωρούμε δεδομένα τα παραδείγματα της δέσμης εκπαίδευσης κι αντιμετωπίζουμε ως μεταβαλλόμενο μέγεθος μόνο τις παραμέτρους του δικτύου. Η συνάρτηση κόστους επομένως είναι μια συνάρτηση:

$$loss = f_X(W) \: : \mathbb{R}^n \rightarrow \mathbb{R}$$

Στόχος μας είναι να βρούμε τις τιμές εκείνες των παραμέτρων $W$ που φτάνουν "κοντά" στην ελάχιστη τιμή της $f$:

\begin{equation}
	|loss_{optimal} - loss_{min}| = |f_X^{optimal}(W)-f_X^{min}(W)|< \varepsilon
	\label{eq:total_minima}
\end{equation}

Ο λόγος που μας ενδιαφέρει η προσέγγιση της ελάχιστης τιμής, κι όχι η εύρεσή της, είναι ότι οι παράμετροι που οδηγούν στην ελάχιστη τιμή αυτή καθ' αυτή, είναι ευάλωτοι σε υπερπροσαρμογή \eng (overfitting). \gre Το ολικό ελάχιστο της συνάρτησης είναι έντονα επηρεασμένο από την τυχαία μορφή των συγκεκριμένων παραδειγμάτων $X$, ενώ η ευρύτερη περιοχή πέριξ αυτού αναπαριστά καλύτερα το γενικό μοτίβο που ακολουθούν τα παραδείγματα εκπαίδευσης και που τελικά θέλουμε το δίκτυό μας να "μάθει". Ο παραπάνω σχολιασμός θα είχε μεγαλύτερη αξία εάν εκπαιδεύαμε το δίκτυο πάνω σε \textbf{όλα} τα δεδομένα του σετ εκπαίδευσης μέσω του αλγορίθμου "απότομης καθόδου" \eng (gradient descent). \gre Η επιλογή μας να χρησιμοποιήσουμε την εναλλακτική μορφή του αλγορίθμου "στοχαστικής απότομης καθόδου μικρής δέσμης" \eng (mini-batch stochastic gradient descent) \gre μας απαλλάσσει από τον παραπάνω προβληματισμό, καθώς το δίκτυο αναπροσαρμόζει τις παραμέτρους του επιλύοντας διαρκώς διαφορετικά προβλήματα ελαχίστου, το καθένα βασισμένο σε ένα διαφορετικό υποσύνολο παραδειγμάτων της συλλογής εκπαίδευσης. Επομένως, είναι σχεδόν βέβαιο ότι με μια μικρή αναπροσαρμογή παραμέτρων στην κατεύθυνση του εκάστοτε ελαχίστου, το δίκτυο δεν θα φτάσει ποτέ ούτως ή άλλως στο ολικό ελάχιστο, παρά μόνο θα το προσεγγίζει, ικανοποιώντας την συνθήκη \ref{eq:total_minima}.

Η συνάρτηση $f_X(W)$ είναι 
\begin{itemize}
	\item μη-κυρτή \eng(non-convex)\gre και μη κοίλη \eng(non-concave)\gre \ref{fig:convex} συνάρτηση
	\item μη-διαφοροποιήσιμη \eng(non-differentiable)\gre
\end{itemize} 


Οι τρεις παραπάνω ιδιότητες προκύπτουν λόγω της συνάρτησης ενεργοποίησης \eng (ReLU) \gre και δυσκολεύουν την προσπάθεια εύρεσης του ολικού ελαχίστου. Δεν υπάρχει κανένα εχέγγυο ότι ο αλγόριθμος "απότομης καθόδου" θα οδηγήσει τις παραμέτρους στο ολικό ελάχιστο. Αντίθετα είναι στατιστικά πιθανότερο να "παγιδευτεί" σε κάποιο από τα τοπικά ελάχιστα. Αυτό ταυτόχρονα σημαίνει ότι για διαφορετικές αρχικές τιμές παραμέτρων το τελικό αποτέλεσμα θα είναι πιθανότατα διαφορετικό, όπως φαίνεται στην εικόνα \ref{fig:non_convex_random}.

H διαισθητική και πειραματικά επιβεβαιωμένη παρατήρηση που διευκολύνει την επίλυση του προβλήματος είναι η εξής. Η συνάρτηση $f$ εμφανίζει σχετικά μικρή απόκλιση ανάμεσα στα τοπικά και στο ολικό της ελάχιστο. Η προσέγγιση ενός τοπικού ελάχιστου δεν θα δώσει ιδιαίτερα χειρότερο αποτέλεσμα από την προσέγγιση του ολικού ελαχίστου. Η παρατήρηση αυτή (μη αποδεδειγμένα μαθηματικά) δίνει αρκετές πιθανότητες η εκπαίδευση του δικτύου να σημειώσει πρόοδο και να καταλήξει σε ένα ικανοποιητικό αποτέλεσμα, εκκινώντας από αρκετά διαφορετικά σημεία. Παρ' όλ' αυτά, η αρχικοποίηση των παραμέτρων κατέχει σημαντικό ρόλο στην εξέλιξη της εκπαίδευσης του δικτύου.

Αρχικοποιύμε το δίκτυο με τις ακόλουθες κατανομές:
\begin{itemize}
	\item οι τιμές των φίλτρων του νευρωνικού δικτύου (βάρη) αρχικοποιούνται με την κατανομή που πρότειναν οι \eng He et. al \gre \citep{He_2015_ICCV}. Η κατανομή αυτή είναι μια κανονική γκαουσιανή την οποία κλιμακώνουμε με την τιμή $\sfrac{2}{n}$, όπου $n$ ο αριθμός των νευρώνων του προηγούμενου επιπέδου. Έτσι εξασφαλίζουμε ότι η κατανομή έχει διακύμανση $\sfrac{2}{n}$ κι η διακύμανση της κατανομής στην έξοδο του συγκεκριμένου επιπέδου θα είναι μοναδιαία.
	\item οι πολώσεις αρχικοποιούνται με μηδενικές τιμές
	\item οι κλιμακώσεις $\gamma$ των επιπέδων κανονικοποίησης δέσμης με μονάδα
	\item οι μετατοπίσεις $\beta$ των επιπέδων κανονικοποίησης δέσμης με μηδενικές τιμές
\end{itemize}

Οι εκπαιδεύσιμες(μεταβλητές) παράμετροι του δικτύου φαίνονται αναλυτικά στον πίνακα \ref{tbl:trainable_parameters}.

\begin{figure}
	\includegraphics[width=\textwidth]{convex_vs_non_convex.png}
	\caption{Παράδειγμα κυρτής (αριστερά) και μη-κυρτής (δεξιά) συνάρτησης.}
	\label{fig:convex}
\end{figure}

\begin{figure}
	\includegraphics[width=\textwidth]{non_convex_random.png}
	\caption{Διαφορετική αρχικοποίηση οδηγεί τον αλγόριθμο "απότομης καθόδου" σε διαφορετικό τελικό αποτέλεσμα.}
	\label{fig:non_convex_random}
\end{figure}

\begin{table}[t]
\eng
\begin{tabular}{l|l|l}
\hline \hline
\multicolumn{3}{c}{\textbf{\gre Συνελικτικό νευρωνικό δίκτυο - Εξαγωγή τοπικού περιγραφέα\eng}} \\ \hline
& \gre Επίπεδο & \gre Εκπαιδεύσιμες παράμετροι \\ \hline \hline
%
\multicolumn{3}{c}{\textbf{block 1}} \\ \hline
1 & \textbf{conv2d} & $\mathtt{patch\_size}^2 \times 1 \times \mathtt{f\_maps}$ \\
2 & \textbf{biases} & $\mathtt{f\_maps}$ \\
3 & \textbf{BN} & $2 \times \mathtt{f\_maps}$ \\
4 & \textbf{ReLU} & $0$ \\ \hline \hline
%
\multicolumn{3}{c}{\textbf{block 2}} \\ \hline
1 & \textbf{conv2d} & $\mathtt{patch\_size}^2 \times \mathtt{f\_maps}^2$\\
2 & \textbf{biases} & $\mathtt{f\_maps}$ \\
3 & \textbf{BN} & $2 \times \mathtt{f\_maps}$ \\
4 & \textbf{ReLU} & $0$ \\ \hline \hline
%
\vdots & \vdots & \vdots \\ \hline 
%
\multicolumn{3}{c}{\textbf{block $\mathtt{n\_blocks}$}} \\ \hline
1 & \textbf{conv2d} & $\mathtt{patch\_size}^2 \times \mathtt{f\_maps}^2$\\
2 & \textbf{biases} & $\mathtt{f\_maps}$ \\
3 & \textbf{BN} & $2 \times \mathtt{f\_maps}$ \\ \hline\hline
%
\multicolumn{3}{c}{\textbf{\gre Δίκτυο απόφασης}} \\ \hline
1 & \textbf{dot product} & $0$\\ \hline \hline
%
\multicolumn{3}{c}{\textbf{\gre Σύνολο}} \\ \hline
\multicolumn{3}{c}{$(\mathtt{patch\_size}^2 \times \mathtt{f\_maps}^2 + 3\mathtt{f\_maps})\mathtt{n\_blocks}$}
\end{tabular}
	\caption{\gre Περίληψη εκπαιδεύσιμων παραμέτρων του νευρωνικού δικτύου.}
	\label{tbl:trainable_parameters}
\gre
\end{table}

\subsubsection{Βελτιστοποίηση των εκπαιδεύσιμων παραμέτρων του δικτύου}
Η εκπαίδευση του δικτύου γίνεται μέσω του βελτιστοποιητή \eng ADAM \gre \citep{kingma2014adam} ....

\subsection{Παρατηρήσεις στο νευρωνικό δίκτυο}

\begin{itemize}
	\item Η αρχικοποίηση των βαρών του δικτύου με διαφορετική κατανομή οδηγεί στο ίδιο περίπου αποτέλεσμα, με παρόμοια καμπύλη εκπαίδευσης.
	\item Η χρήση του επιπέδου κανονικοποίησης δέσμης έχει τα εξής βασικά πλεονεκτήματα:
	\begin{itemize}
		\item Επιταχύνει έντονα την εκπαίδευση, επιτρέποντας ιδιαίτερα υψηλούς δείκτες εκμάθησης \eng(learning rates).\gre
		\item 
	\end{itemize}
\end{itemize}


